{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Spark?\n",
    "**1. Speed:** Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.\n",
    "\n",
    "**2. Ease of Use**\n",
    "\n",
    "**3. Generality:** Combine SQL, streaming, and complex analytics.\n",
    "\n",
    "**4. Runs Everywhere:** Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.\n",
    "\n",
    "Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. You can combine these libraries seamlessly in the same application.\n",
    "![](./image/stack.png)\n",
    "\n",
    "\n",
    "## Demo Code in this Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## set up  SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "# when set to true, the first line of files name columns and are not included in data.\n",
    "# automatically infer column types. It requires one extra pass over the data and is false by default.\n",
    "df = spark.read.format('csv'). \\\n",
    "        options(header='true', inferschema='true').load(\"../data/Advertising.csv\",header=True)\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Components\n",
    "![](./image/spark-components.png)\n",
    "\n",
    "1. Spark Driver\n",
    "\n",
    "    * separate process to execute user applications\n",
    "    \n",
    "    \n",
    "    * creates SparkContext to schedule jobs execution and negotiate with cluster manager\n",
    "\n",
    "2. Executors\n",
    "\n",
    "    * run tasks scheduled by driver\n",
    "    * store computation results in memory, on disk or off-heap\n",
    "    * interact with storage systems\n",
    "\n",
    "3. Cluster Manager\n",
    "\n",
    "    * Mesos\n",
    "    * YARN\n",
    "    * Spark Standalone\n",
    "\n",
    "+ Spark Driver contains more components responsible for translation of user code into actual jobs executed on cluster:\n",
    "\n",
    "![](./image/spark-components.png)\n",
    "\n",
    "represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster\n",
    "\n",
    "+ DAGScheduler\n",
    "\n",
    "computes a DAG of stages for each job and submits them to TaskScheduler determines preferred locations for tasks (based on cache status or shuffle files locations) and finds minimum schedule to run the jobs\n",
    "\n",
    "+ TaskScheduler\n",
    "\n",
    "responsible for sending tasks to the cluster, running them, retrying if there are failures, and mitigating stragglers\n",
    "\n",
    "+ SchedulerBackend\n",
    "\n",
    "backend interface for scheduling systems that allows plugging in different implementations(Mesos, YARN, Standalone, local)\n",
    "\n",
    "+ BlockManager\n",
    "\n",
    "provides interfaces for putting and retrieving blocks both locally and remotely into various stores (memory, disk, and off-heap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Spark Works?\n",
    "\n",
    "Spark has a small code base and the system is divided in various layers. Each layer has some responsibilities. The layers are independent of each other.\n",
    "\n",
    "\n",
    "The first layer is the interpreter, Spark uses a Scala interpreter, with some modifications. As you enter your code in spark console (creating RDD’s and applying operators), Spark creates a operator graph. When the user runs an action (like collect), the Graph is submitted to a DAG Scheduler. The DAG scheduler divides operator graph into (map and reduce) stages. A stage is comprised of tasks based on partitions of the input data. The DAG scheduler pipelines operators together to optimize the graph. For e.g. Many map operators can be scheduled in a single stage. This optimization is key to Sparks performance. The final result of a DAG scheduler is a set of stages. The stages are passed on to the Task Scheduler. The task scheduler launches tasks via cluster manager. (Spark Standalone/Yarn/Mesos). The task scheduler doesn’t know about dependencies among stages.\n",
    "\n",
    "![](./image/work_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD represents Resilient Distributed Dataset. An RDD in Spark is simply an immutable distributed collection of objects sets. Each RDD is split into multiple partitions (similar pattern with smaller sets), which may be computed on different nodes of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RDD\n",
    "Usually, there are two popular ways to create the RDDs: loading an external dataset, or distributing a set of collection of objects. The following examples show some simplest ways to create RDDs by using parallelize() fucntion which takes an already existing collection in your program and pass the same to the Spark Context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By using parallelize( ) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+\n",
      "|col1|col2|col3| col4|\n",
      "+----+----+----+-----+\n",
      "|   1|   2|   3|a b c|\n",
      "|   4|   5|   6|d e f|\n",
      "|   7|   8|   9|g h i|\n",
      "+----+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark create RDD example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.sparkContext.parallelize([\n",
    "            (1, 2, 3, 'a b c'),\n",
    "             (4, 5, 6, 'd e f'),\n",
    "             (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parallelize() distribute a local python collection to form an RDD. Common built-in python collections include dist, list, tuple or set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from a list\n",
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog', 'fish']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from a tuple\n",
    "rdd = sc.parallelize(('cat', 'dog', 'fish'))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 'dog', 'fish'), ('orange', 'apple')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from a list of tuple\n",
    "list_t = [('cat', 'dog', 'fish'), ('orange', 'apple')]\n",
    "rdd = sc.parallelize(list_t)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fish', 'dog', 'cat']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from a set\n",
    "s = {'cat', 'dog', 'fish', 'cat', 'dog', 'dog'}\n",
    "rdd = sc.parallelize(s)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from a dict\n",
    "d = {\n",
    "    'a': 100,\n",
    "    'b': 200,\n",
    "    'c': 300\n",
    "}\n",
    "rdd = sc.parallelize(d)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Python Spark create RDD example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "myData = spark.sparkContext.parallelize([(1,2), (3,4), (5,6), (7,8), (9,10)])\n",
    "myData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By using createDataFrame( ) function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id='1', Name='Joe', Sallary='70000', DepartmentId='1'),\n",
       " Row(Id='2', Name='Henry', Sallary='80000', DepartmentId='2'),\n",
       " Row(Id='3', Name='Sam', Sallary='60000', DepartmentId='2'),\n",
       " Row(Id='4', Name='Max', Sallary='90000', DepartmentId='1')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark create RDD example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "Employee = spark.createDataFrame([\n",
    "                        ('1', 'Joe',   '70000', '1'),\n",
    "                        ('2', 'Henry', '80000', '2'),\n",
    "                        ('3', 'Sam',   '60000', '2'),\n",
    "                        ('4', 'Max',   '90000', '1')],\n",
    "                        ['Id', 'Name', 'Sallary','DepartmentId']\n",
    "                       )\n",
    "Employee.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By using read and load functions\n",
    "1. Read dataset from .csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(x=[1, 2, 3], y=['a', 'b', 'c']), Row(x=[4, 5, 6], y=['e', 'f', 'g'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd = sc.parallelize([\n",
    "    Row(x=[1,2,3], y=['a','b','c']),\n",
    "    Row(x=[4,5,6], y=['e','f','g'])\n",
    "])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|        x|        y|\n",
      "+---------+---------+\n",
      "|[1, 2, 3]|[a, b, c]|\n",
      "|[4, 5, 6]|[e, f, g]|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(rdd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[a, b, c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4, 5, 6]</td>\n",
       "      <td>[e, f, g]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x          y\n",
       "0  [1, 2, 3]  [a, b, c]\n",
       "1  [4, 5, 6]  [e, f, g]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf = pd.DataFrame({\n",
    "    'x': [[1,2,3], [4,5,6]],\n",
    "    'y': [['a','b','c'], ['e','f','g']]\n",
    "})\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up  SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark create RDD example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "|  8.7| 48.9|     75.0|  7.2|\n",
      "| 57.5| 32.8|     23.5| 11.8|\n",
      "|120.2| 19.6|     11.6| 13.2|\n",
      "|  8.6|  2.1|      1.0|  4.8|\n",
      "|199.8|  2.6|     21.2| 10.6|\n",
      "| 66.1|  5.8|     24.2|  8.6|\n",
      "|214.7| 24.0|      4.0| 17.4|\n",
      "| 23.8| 35.1|     65.9|  9.2|\n",
      "| 97.5|  7.6|      7.2|  9.7|\n",
      "|204.1| 32.9|     46.0| 19.0|\n",
      "|195.4| 47.7|     52.9| 22.4|\n",
      "| 67.8| 36.6|    114.0| 12.5|\n",
      "|281.4| 39.6|     55.8| 24.4|\n",
      "| 69.2| 20.5|     18.3| 11.3|\n",
      "|147.3| 23.9|     19.1| 14.6|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = spark.read.format('csv').\\\n",
    "                               options(header='true', \\\n",
    "                               inferschema='true').\\\n",
    "                load(\"../data/Advertising.csv\")\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textFile()\n",
    "The textFile() function reads a text file and returns it as an RDD of strings. Usually, you will need to apply some map functions to transform each elements of the RDD to some data structure/type that is suitable for data analysis.\n",
    "\n",
    "When using textFile(), each line of the text file becomes an element in the resulting RDD.\n",
    "\n",
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',mpg,cyl,disp,hp,drat,wt,qsec,vs,am,gear,carb',\n",
       " 'Mazda RX4,21,6,160,110,3.9,2.62,16.46,0,1,4,4',\n",
       " 'Mazda RX4 Wag,21,6,160,110,3.9,2.875,17.02,0,1,4,4',\n",
       " 'Datsun 710,22.8,4,108,93,3.85,2.32,18.61,1,1,4,1',\n",
       " 'Hornet 4 Drive,21.4,6,258,110,3.08,3.215,19.44,1,0,3,1']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read a csv file\n",
    "rdd = sc.textFile('../data/mtcars.csv')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fresh install of XP on new computer. Sweet relief! fuck vista\\t1018769417\\t1.0',\n",
       " 'Well. Now I know where to go when I want my knives. #ChiChevySXSW http://post.ly/RvDl\\t10284216536\\t1.0']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read a txt file\n",
    "rdd = sc.textFile('../data/twitter.txt')\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
