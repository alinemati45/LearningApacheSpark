{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up spark context and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark regression example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               TV|             Radio|         Newspaper|             Sales|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|              200|               200|               200|               200|\n",
      "|   mean|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "                       options(header='true', \\\n",
    "                       inferschema='true').\\\n",
    "            load(\"../data/Advertising.csv\",header=True);\n",
    "df.show(5,True)\n",
    "df.printSchema()\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data to dense vector (features and label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[230.1,37.8,69.2]| 22.1|\n",
      "| [44.5,39.3,45.1]| 10.4|\n",
      "| [17.2,45.9,69.3]|  9.3|\n",
      "|[151.5,41.3,58.5]| 18.5|\n",
      "|[180.8,10.8,58.4]| 12.9|\n",
      "+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# I provide two ways to build the features and labels\n",
    "\n",
    "# method 1 (good for small feature):\n",
    "#def transData(row):\n",
    "#    return Row(label=row[\"Sales\"],\n",
    "#               features=Vectors.dense([row[\"TV\"],\n",
    "#                                       row[\"Radio\"],\n",
    "#                                       row[\"Newspaper\"]]))\n",
    "\n",
    "# Method 2 (good for large features):\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])\n",
    "\n",
    "transformed= transData(df)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal With Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----------------+\n",
      "|         features|label|  indexedFeatures|\n",
      "+-----------------+-----+-----------------+\n",
      "|[230.1,37.8,69.2]| 22.1|[230.1,37.8,69.2]|\n",
      "| [44.5,39.3,45.1]| 10.4| [44.5,39.3,45.1]|\n",
      "| [17.2,45.9,69.3]|  9.3| [17.2,45.9,69.3]|\n",
      "|[151.5,41.3,58.5]| 18.5|[151.5,41.3,58.5]|\n",
      "|[180.8,10.8,58.4]| 12.9|[180.8,10.8,58.4]|\n",
      "+-----------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=4).fit(transformed)\n",
    "\n",
    "data = featureIndexer.transform(transformed)\n",
    "data.show(5,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets (40% held out for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|        features|label|\n",
      "+----------------+-----+\n",
      "|  [4.1,11.6,5.7]|  3.2|\n",
      "| [7.8,38.9,50.6]|  6.6|\n",
      "|   [8.6,2.1,1.0]|  4.8|\n",
      "|[11.7,36.9,45.2]|  7.3|\n",
      "| [17.2,4.1,31.6]|  5.9|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|       features|label|\n",
      "+---------------+-----+\n",
      "| [0.7,39.6,8.7]|  1.6|\n",
      "| [5.4,29.9,9.4]|  5.3|\n",
      "|[7.3,28.1,41.4]|  5.5|\n",
      "| [8.4,27.2,2.1]|  5.7|\n",
      "|[8.7,48.9,75.0]|  7.2|\n",
      "+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = transformed.randomSplit([0.6, 0.4])\n",
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Model:\n",
    "    \n",
    "Due to the sparsity within our data, our training sets will often be ill-posed (singular). Applying regularization to the regression has many advantages, including:\n",
    "\n",
    "Converting ill-posed problems to well-posed by adding additional information via the penalty parameter \\lambda\n",
    "Preventing overfitting\n",
    "Variable selection and the removal of correlated variables (Glmnet Vignette). The Ridge method shrinks the coefficients of correlated variables while the LASSO method picks one variable and discards the others. The elastic net penalty is a mixture of these two; if variables are correlated in groups then \\alpha=0.5 tends to select the groups as in or out. If Î± is close to 1, the elastic net performs much like the LASSO method and removes any degeneracies and wild behavior caused by extreme correlations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary least squares regression (L2 penalty)\n",
    "![](./image/ols.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: the last rows are the information for Intercept\n",
      "## -------------------------------------------------\n",
      "##   Estimate   |   Std.Error | t Values  |  P-value\n",
      "##   0.042992   0.001614   26.643   0.000000\n",
      "##   0.195859   0.009646   20.305   0.000000\n",
      "##  -0.000059   0.006570   -0.009   0.992864\n",
      "##   3.348187   0.355384    9.421   0.000000\n",
      "## ---\n",
      "## Mean squared error:  2.064104 ,RMSE:  1.436699\n",
      "## Multiple R-squared: 0.926180 , Total iterations: 1\n",
      "+---------------+-----+------------------+\n",
      "|       features|label|        prediction|\n",
      "+---------------+-----+------------------+\n",
      "| [0.7,39.6,8.7]|  1.6|11.133777564563786|\n",
      "| [5.4,29.9,9.4]|  5.3|  9.43596687661333|\n",
      "|[7.3,28.1,41.4]|  5.5| 9.163220782315854|\n",
      "| [8.4,27.2,2.1]|  5.7| 9.036553054806895|\n",
      "|[8.7,48.9,75.0]|  7.2|13.295293560104216|\n",
      "+---------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 2.01457\n",
      "r2_score: 0.8345527321449107\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "# lr =LinearRegression(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", maxIter=100,\n",
    "# regParam=0.1, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, standardization=True, solver=\"auto\",\n",
    "# weightCol=None, aggregationDepth=2)\n",
    "\n",
    "lr =LinearRegression(featuresCol=\"features\", labelCol=\"label\", \n",
    "                     predictionCol=\"prediction\", maxIter=100,\n",
    "                        regParam=0.1, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \n",
    "                     standardization=True, solver=\"auto\",\n",
    "                     aggregationDepth=2)\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, lr])\n",
    "\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "def modelsummary(model):\n",
    "    import numpy as np\n",
    "    print (\"Note: the last rows are the information for Intercept\")\n",
    "    print (\"##\",\"-------------------------------------------------\")\n",
    "    print (\"##\",\"  Estimate   |   Std.Error | t Values  |  P-value\")\n",
    "    coef = np.append(list(model.coefficients),model.intercept)\n",
    "    Summary=model.summary\n",
    "\n",
    "    for i in range(len(Summary.pValues)):\n",
    "        print (\"##\",'{:10.6f}'.format(coef[i]),\\\n",
    "        '{:10.6f}'.format(Summary.coefficientStandardErrors[i]),\\\n",
    "        '{:8.3f}'.format(Summary.tValues[i]),\\\n",
    "        '{:10.6f}'.format(Summary.pValues[i]))\n",
    "\n",
    "    print (\"##\",'---')\n",
    "    print (\"##\",\"Mean squared error: % .6f\" \\\n",
    "           % Summary.meanSquaredError, \",RMSE: % .6f\" \\\n",
    "           % Summary.rootMeanSquaredError )\n",
    "    print (\"##\",\"Multiple R-squared: %f\" % Summary.r2, \", Total iterations: %i\"% Summary.totalIterations)\n",
    "modelsummary(model.stages[-1])\n",
    "\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\",\"prediction\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {0}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression (L1 penalty)\n",
    "\n",
    "![](./image/ridge.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: the last rows are the information for Intercept\n",
      "## -------------------------------------------------\n",
      "##   Estimate   |   Std.Error | t Values  |  P-value\n",
      "##   0.043722   0.001627   26.873   0.000000\n",
      "##   0.199257   0.009743   20.452   0.000000\n",
      "##  -0.001022   0.006633   -0.154   0.877801\n",
      "##   3.184361   0.356948    8.921   0.000000\n",
      "## ---\n",
      "## Mean squared error:  2.057158 ,RMSE:  1.434280\n",
      "## Multiple R-squared: 0.926428 , Total iterations: 1\n",
      "+---------------+-----+------------------+\n",
      "|       features|label|        prediction|\n",
      "+---------------+-----+------------------+\n",
      "| [0.7,39.6,8.7]|  1.6| 11.09665927920621|\n",
      "| [5.4,29.9,9.4]|  5.3| 9.368643175490465|\n",
      "|[7.3,28.1,41.4]|  5.5|  9.06034419947503|\n",
      "| [8.4,27.2,2.1]|  5.7| 8.969276849953975|\n",
      "|[8.7,48.9,75.0]|  7.2|13.231761660623357|\n",
      "+---------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 2.00698\n",
      "r2_score: 0.8357977088685933\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "# lr =LinearRegression(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", maxIter=100,\n",
    "# regParam=0.1, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, standardization=True, solver=\"auto\",\n",
    "# weightCol=None, aggregationDepth=2)\n",
    "\n",
    "lr =LinearRegression(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \n",
    "                    maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, \n",
    "                fitIntercept=True, standardization=True, solver=\"auto\", aggregationDepth=2)\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, lr])\n",
    "\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "def modelsummary(model):\n",
    "    import numpy as np\n",
    "    print (\"Note: the last rows are the information for Intercept\")\n",
    "    print (\"##\",\"-------------------------------------------------\")\n",
    "    print (\"##\",\"  Estimate   |   Std.Error | t Values  |  P-value\")\n",
    "    coef = np.append(list(model.coefficients),model.intercept)\n",
    "    Summary=model.summary\n",
    "\n",
    "    for i in range(len(Summary.pValues)):\n",
    "        print (\"##\",'{:10.6f}'.format(coef[i]),\\\n",
    "        '{:10.6f}'.format(Summary.coefficientStandardErrors[i]),\\\n",
    "        '{:8.3f}'.format(Summary.tValues[i]),\\\n",
    "        '{:10.6f}'.format(Summary.pValues[i]))\n",
    "\n",
    "    print (\"##\",'---')\n",
    "    print (\"##\",\"Mean squared error: % .6f\" \\\n",
    "           % Summary.meanSquaredError, \",RMSE: % .6f\" \\\n",
    "           % Summary.rootMeanSquaredError )\n",
    "    print (\"##\",\"Multiple R-squared: %f\" % Summary.r2, \", Total iterations: %i\"% Summary.totalIterations)\n",
    "modelsummary(model.stages[-1])\n",
    "\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\",\"prediction\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {0}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Elastic net ( the penalty is an L1 + L2 penalty)\n",
    "\n",
    "![](./image/Elastic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: the last rows are the information for Intercept\n",
      "## -------------------------------------------------\n",
      "##   Estimate   |   Std.Error | t Values  |  P-value\n",
      "##   0.043722   0.001627   26.873   0.000000\n",
      "##   0.199257   0.009743   20.452   0.000000\n",
      "##  -0.001022   0.006633   -0.154   0.877801\n",
      "##   3.184361   0.356948    8.921   0.000000\n",
      "## ---\n",
      "## Mean squared error:  2.057158 ,RMSE:  1.434280\n",
      "## Multiple R-squared: 0.926428 , Total iterations: 1\n",
      "+---------------+-----+------------------+\n",
      "|       features|label|        prediction|\n",
      "+---------------+-----+------------------+\n",
      "| [0.7,39.6,8.7]|  1.6| 11.09665927920621|\n",
      "| [5.4,29.9,9.4]|  5.3| 9.368643175490465|\n",
      "|[7.3,28.1,41.4]|  5.5|  9.06034419947503|\n",
      "| [8.4,27.2,2.1]|  5.7| 8.969276849953975|\n",
      "|[8.7,48.9,75.0]|  7.2|13.231761660623357|\n",
      "+---------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 2.00698\n",
      "r2_score: 0.8357977088685933\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "lr =LinearRegression(featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \n",
    "                     maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, \n",
    "                     fitIntercept=True, standardization=True, solver=\"auto\",\n",
    "                     aggregationDepth=2)\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, lr])\n",
    "\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "def modelsummary(model):\n",
    "    import numpy as np\n",
    "    print (\"Note: the last rows are the information for Intercept\")\n",
    "    print (\"##\",\"-------------------------------------------------\")\n",
    "    print (\"##\",\"  Estimate   |   Std.Error | t Values  |  P-value\")\n",
    "    coef = np.append(list(model.coefficients),model.intercept)\n",
    "    Summary=model.summary\n",
    "\n",
    "    for i in range(len(Summary.pValues)):\n",
    "        print (\"##\",'{:10.6f}'.format(coef[i]),\\\n",
    "        '{:10.6f}'.format(Summary.coefficientStandardErrors[i]),\\\n",
    "        '{:8.3f}'.format(Summary.tValues[i]),\\\n",
    "        '{:10.6f}'.format(Summary.pValues[i]))\n",
    "\n",
    "    print (\"##\",'---')\n",
    "    print (\"##\",\"Mean squared error: % .6f\" \\\n",
    "           % Summary.meanSquaredError, \",RMSE: % .6f\" \\\n",
    "           % Summary.rootMeanSquaredError )\n",
    "    print (\"##\",\"Multiple R-squared: %f\" % Summary.r2, \", Total iterations: %i\"% Summary.totalIterations)\n",
    "modelsummary(model.stages[-1])\n",
    "\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\",\"prediction\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {0}'.format(r2_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
